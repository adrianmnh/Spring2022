![](331-img/break.png)

# *Class 1 - 2/1/2022*




![](331-img/break.png)

# *Class 2 - 2/3/2022*

IBM:
* 30% less to design and build **Containerized data center**
* Cheaper than air conditionining dropped ceilings/raised floors

## Cloud Deployment mode

### Public Cloud
Cloud infrastructure available to the **general public**, owned by organization selling cloud services
* Large scale on a rental basis
  * Access remotely under some Service Level Agreement **(SLAs)**
  * usually has global network of data centers
* Accessed from webservices or APIs
* E-commerce accountability
  * Web-based, utility style costing
  * "Pas-as-you-go"

### Private Cloud
Infrastructure for **single organization only**, may be managed by the org or a 3rd party(Virtual Private Cloud), hosted internally or externally

Key techniques
* Virtualization techniques(VMWare, Xen, KVM) 
* Virtual Private network(VPN)

`More expensive, more secure(behind firewall)`
* Still have to buy, build and manage
* May not free from the responsibility for procurring (hardware+software upgrade$$) and maintenance(in-house expertise $$$)

> New York Times "Time Machine": 15 million articles were put into the clous on servers owned by Amazon

* Open-source Software for building private and public clouds: **Open Stack** and **Apache CLoudStack**

### Community Cloud
Shared by several organizations that have shared concerns

Some users may required different levels of security

* High Security: Access to cloud is granted only after a trusted validation of identity(required by regulating bodies)

* High Availability: Resources are 99.99% available(or better): **Banking** and **Military**

* High Performance: Optimized for high transaction rates and extremely low-latency: **High Frequency Trading(HFT)**

**GovernmentClouds**

Government organizations may share computing infrastructure on the cloud to manage data related to citizens: **Amazon AWS GovCloud** and **IBM SmartCloud** for government(SCG)

**Cloud for High Performance Computing**

A different set of requirements
* Close to the 'metal'
* User-space communication(bypass OS)
* high-speed interconnect: **InfiniBand**

Clients share a common set of "Big Data" - ranging in size from Terabytes to Petabyes. Need a very fast I/O subsystem (SSD-based storage)

**Financial Services Clouds**

Require microseconds or at least milliseconds of response time and latency measurements

**NYSE Technologies: Financial Services Community Cloud**(electronic trading, maket data analysis)

**Hybrid Clouds**

Integrated cloud service utilizing both private and public clouds to perform distinct functions within the same organization

* non-sensitive operations on public cloud, and sensitive operations handled in-house(private)
* bound by standardized or proprietary technilogy that enables 

<img src="./cloud-img/02-01.png">

TPU Tensor Processing Units


<img src="./cloud-img/break.png">

# *Class 3 - 2/10/2022*
## Availability Zones
**Unique physical locations** within an Azure region. Made up of **one or more data centers** equipped with **independent power, cooling,and networking**. To ensure resiliency, there's a minimum of three separete zones in all enable regions

The physical separtion of Availability Zones within a region protects applications and data from **datacenter failures**. Zone-redundant services replicate your apps and data across Availability Zones to protect from single-points-of-failure.

You can **synchronously** replicate your apps and data using Availability Zones for **high-availability** and **asynchronously** replicate across Azure regions for **disaster recovery protection**

<img src="./cloud-img/04-01.png" width =200>

## Project Natick
long lived, resilient data centers that operate *lights out*, nobody on site, no maintenance for years.
* June 2008
  * 12 racks
  * 864 servers
  * 27.6 petabytes of storage
  
Retrieved for analysys after more than 2 years at the bottom of the ocean

Power consumption: 240 kW generated by on-shore wing and solar along with off-shore tide and wave power

Data shows this approach improves performance and reliability of the datacenter when compared to land. **Project Natick had 1/8th the failure rate of land data centers.** Not only is a greener future possible, but it is economically practical.

This is due to the atmosphere of **nitrogen**, which is less corrosive than oxygen, and the **absence of people to bump and jostle** components, are the primary reasons for the difference

Operating with a highly efficient **PUE** of **1.07**
>power usage effectiveness = total power / server power; lower values are better, 1.0 is perfect

## Google's data center
Around **260 million watts** of power is used by Google data centers, enough to consistently power 200,000 homes

### data center @Okalahoma
Hundreds of fans tunnel hot air from the server racks into a cooling unit to be recirculated in. The **green lights** are the server status LEDs reflecting 


## Facebook's data center
Doesn't use traditional air conditioning, instead relying completly on outside air.

Power Usage Effectiveness @Prineville 

## AWS's data centers
The AWS Cloud spans 84 Availability Zones within 26


<img src="./cloud-img/break.png">

# *Class 4 - 2/15/2022*



<img src="./cloud-img/break.png">

# *Class 7 - 2/24/2022*
## Everything at google runs in cointainers
* Gmail Webserach Maps
* MapReduce MillWHeel Pregel
* Colossus BigTable SPanner
* Even Gogogle Cloud COmputing

## Container Orchestration
`More than packing and isolation needed`

Scheduling?

Lifecycle and health?

Discovery?

Monitoring?

Auth{n,z}

Aggregates?

Scaling?

Containers working together have dependencies.

## Open Source Containers: **Kubernetes**

Greek for Helmsamn; also the root of the word Governor and cybernetic
* Container orchestrator
* Builds on Docker containers
  * also supporting other container technologies
* Multiple cloud and bare-metal environments
* Supports existing OSS apps
  * cannot require appds becoming cloud-native
* Inspired and informed by Google's experiences and internal systems
* **100% Open source**, written in **GO**

Let users manage applications, not machines
> Look at Borg and its connection with Kubernetes


![](./cloud-img/07-01.png)

Basic Unit of container: Pod

## Pod
A kubernetes abstraction represents a group of one or more application containers, and some shared resources for those containers

* Shared storage, as Volumes
* Networking, as a unique cluster IP address
* Information about how to run each container, such as the container image version or specific ports to use

![](./cloud-img/07-02.png)

Containers can be associated with different labels

## Pod Lifecycle
Once schedule to a node, pods do not move
* restart policy means restart **in-place**

Pods can be ovserved _pending, running, succeeded, or failed_
* _failed_ is **really** the end - no more restarts
* no complex state machine logic

Pods are **not rescheduled** by the scheduler or apiserver
* Even if a  node dies
* controllers are responsible for this
* keeps the scheduler **simple**

Appds should consider there rules
* Services hide this? `services don't show pods prop`
* Makes pod-to-pod communication more formal

## Pod

>Simplest unit in Kubernetes
* Represents Process running in your cluster
* Encapsulates a container(or sometimes multiple)
* Replicating a Pod serves to scale and application horizontally

![](./cloud-img/07-03.png)

## ReplicaSet

>Maintains a set of identical Pods
* Definition consists of:
  * Number of replicas
  * Pod Template
  * Selectyor to identify which Pods it can acquire
> Generally encapsulated by a Deployment

![](./cloud-img/07-04.png)

## Deployment

> Provides updates for pods and replicasets
* Runs multiple replicas of your application
* Suitable for stateless applications

![](./cloud-img/07-05.png)

## Control Plane

### Kube-apiserver

> Provides a forward facing REST interface into the Kubernetes control plane and datastore

* All clients and other applications interact with KLubernetes **strictly** through the API Server
* Acts as the gatekleeper to the cluster by handling authentication and authorization, request validation, mutation, and admission control in addition to being the front-end to the backing datastore

### Kube-controller-manager
* Monitors the cluster state via the apiserver and **steers the cluster towards the desired state**
* **Node Controller**: Responsible for noticing and responding when nodes go down
* **Replication Controller**: Responsible for maintaining the correct number of pods for every application controller object in the system
* **Endpoints Controller**: Populates the Endpoints object(that is, joins Services & Pods).
* **Service Account & Token Controllers**: Crewate default accounts and API access tokens for 

### kube-scheduler
* Component on the master that watches newly created pods that have no node assigned, and selects a node for them to run on
* Factors taken into account for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and antiaffinity specifications, data locality, inter-workload interference and deadlines

### cloud-controller-manager
* **Node Controller**: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops repsonding
* **Route Controller**: For setting up routes in the underlying cloud infrastructure
* **Service Controller**: 
* **4**: 

### etcd
* Atomic key-value store that uses Raft consensus
* Backing store for all control plane metadata


## Reconciliation between de lared and actual state

## Control loops

## Replication Controllers
> A type of controllers(control loop)
* Ensures N copies of a pod always running
  * if too few, start new ones
  * if too many, kill some
  * group == selector
* Cleanly layered on top of the core
  * all access is by puclic APIs
* Replicated pods are fungible
  * ..

## Node Components

### Kubelet
* agent(daemon) that runs on each node in the cluster. It makes sure that containers are running in a pod
* The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy

### kube-proxy
* Managet the network rules on each node
* Perform connection forwarding or load balancing for Kubernetes kluster

### Container Runtime Engine
* A container runtin is a CRI(COntainer Runtime Interface) compatible application that executes and manages containers
  * Containered(docker)
  * Cri-o
  * Rkt
  * Kata(formerly known as )

# Borg
CLuster management system at Google taht achieves high utilization by:
* Admission control
* Efficient task-packing
* Over-commitment
* Machine sharing

## User Perspective
* Allocs
  * Reserved set of resources
* Priority, Quota, and Admission Control
  * Job has a priority

## Scheduling a Job
```go
job Hello_world = {
  runtime = { cell = "ic" } // what cell shoyuld run it in?
}
```

## Borg Architecture
* Borgmaster
  * main Borgmaster process & Scheduler
  * Five Replicas
* Borglet

## Kubernetes
**Directly derived:**
* Borglet ⇒ Kubelet
* alloc ⇒ pod
* Borg containers ⇒ docker
* declarative specifications

**Loosely inpired by Borg**
* Job ⇒ labels
* managed ports ⇒ IP per pod
* Monolithic master ⇒ micro-services

![](./cloud-img/07-06.png)

next week: **Hadoop**

<img src="./cloud-img/break.png">

# *Class 8 - 3/1/2022*

# Hadoop

Sense of Data Size

1 byte = 8 bits

Kilobyte(1000 bytes) = 10^3bytes

Megabyte = 10^6. One Photo image (iPhone) 4.5 MB

Gigabyte(GB) = 10^9 bytes. 8/16 GB DRAM in MacBook

Terabyte 10^12 Your hand disk: 1-3TB

Petabyte(PB) = 10^15 bytes

223,000 DVDs 4.7GB each to hold 1PB

Ober 3.4 years of 24/7 Full HD video recording = 1 PB

![](./cloud-img/08-01.png)



## Apache Hadoop
Compose of the following modules:

* Hadoop Common - contains libraries and utilities needed by other hadoop modules

* Hadoop Distributed File System (HDFS) - a distributed file system for storing data

* Hadoop MapReduce - a programming model for large scale data processing

* Hadoop YARN - resource-management and task scheduling

* Hadoop Ozone(new in Hadoop )

* ...

![](./cloud-img/08-02.png)

# Hadoop 1.x Architecture
![](./cloud-img/08-03.png)

# Hadoop 1.x key components

## Name Node @ master node

* Stores metadata : file/chunk namespaces, file-to-chunk mapping, location of each chunk's replicas

## Job Tracker @ master node
* Keeps track of all Map Redune Jobs that are running on various nodes.

## Task Tracker @ each slave node
* a slave tracker to the Job Tracker
* Launches child processes (JVMs) to execute ....

# Hadoop 2.x (Change in Hadoop Architecture)

![](cloud-img/08-04.png)

# Hadoop 2.x key componenets
In Hadoop 2.x, there is no Job Tracker and Task Tracker. Replaced by (1)Resource Manager, (2)Node Manager, (3)Application Manager(more in Yarn)

* Roles of the cluster nodes:

  * Master Node(s): Typically one machine in the cluster is designated as the nameNide(NN) and another machine as the Resource Manager(RM), exclusively
    * For simplicity, we can ...

# Hadoop 2.x: HDFS + Yarn
HDFS(Hadoop File System): store your data
* Name Node + N DataNodes

Yarn 

# Hadoop 2.x: HDFS + Yarn
![](cloud-img/08-05.png)

# Hadoop in the cloud

* Microsoft Azyure: Azure HDInsight
* Amazon Elastic MapReduce(EMR)
* Google Cloud Platform
  * Google Cloud Dataproc: PaaS running Apache Spark and Apache Hadoop clusters

* Oracle Cloud Platform
  * Oracle Big Data SQL Cloud Service

# Google File System
> `Goal: global(distributed) file system that stores data across many machines`
  * Need to handle 100's TBs
* Google published details in 2003
* **Open Source**

# Workload-driven design
>`Google workload characteristics - Huge files (GBs)`
* Almost all writes are appends
* Concurrent appends common
* High throughput is valuable
* Low latency is not

# Workload examples
Read entire dataset, do computation over it

* Producer/consumer: many producers append work to file concurrently; one consumer reads and does work

# Workload-driven design
> Build a gloval (distributed) file system that incorporates all these application properties
* Only supports features required by applications
* Avoid difficult local system features, e.g.:
  * rename dir
  * links

# Design details
![](cloud-img/08-06.png)
* `Files Stored as blocks`
  * HDFS file is chopped up into 64MB/128MB blocks
  * each block will reside on a different datanode
* `Single master to coordinate access, keep metadata`
  * Simple centralized master per Hadoop cluster
  * Manages metadata(doesn't store the actual data chunks)
  * Periodic heatbeat messages to check up on slaves 

# Reliability
* "Hardware failure is the norm rather than the exception"
  * Hundreds of thousands of machines/disks(cheap but unreliable)
  * Each componenet has a non-trivial probability of failure
  * 100K drives(MTBF=3 years). one "failure" every 15 minutes
  * Add in H/W failures for network, memory, power, etc
* Reliability through replication
  * Each block is replicated across 3+ ........
  
<sub>*Mean Time Between Failures</sub>

# HDFS - A Quick Summary
* HDFS is written in Java
* Scaled to tens of petabytes of storage
* Files split into blocks(default 64 or 128 MB), replicated across several data nodes(default 2) for fault tolerance
* Single name...................

# Writting files to HDFS
* The client breaks File.txt into Blocks(3 blocks: A, B, C)
  * For each block, Client consuslts NameNode
  * Cleints writes block directly to one DataNode
  * DataNode replicates block(not shown)
![](cloud-img/08-08.png)

# Replica placement policy
* (1)Network performance issue:
  * Communication in-rack: higher bandwith, lower latency(good for performance)
  * Keep bulky flows in-rack when possible
* (2)Data loss prevention
  * Never lose all data even when the entire rack fails
* Improve data reliability.....
![](cloud-img/08-09.png)

# Replica placement policy
* **Replication factor = 3**
  * **1st one** on the same node as the client(the writer),. otherwise on a random datanode
  * **2nd** on a node in a different rack(off-rack)
  * **3rd** on the same rack as 2nd, but on a different node
* Why so?
  * Tradeoff between reliability, write bandwidth and read bandwidth

# Writing replicas
![](cloud-img/08-10.png)
![](cloud-img/08-11.png)
When completed, each datanode reports to namenode "block received" with block info
![](cloud-img/08-12.png)
Note: The initial node of the subsequent blocks of File.txt will vary for each block (why?) Spreading around the hot spots of in-rack and across-rack traffic
![](cloud-img/08-13.png)


![](./cloud-img/break.png)

# *Class 9 - 3/3/2022*

# Discussion
* Why not put them in Three nodes located at three different racks?
* Seem Good: This maximizes redundancy.......

To maximize reliability, we can put them in 3 different racks. Sending data from from one rack over ToR switches

## Heartbeats & block reports
Data node sends Heartbeats to Name Node every 3 seconds

Every 6 hours is a (full) block report 

* Blocks reports provide the NameNode with an up-to data view of where block replicas are located on the cluster

NameNode builds meta data from Block reports


## Real-world use case of HDFS

NetApp provides storage solution to business/companies

Large financial firm: 60 PB of raw data

Requires 1200 HDFS storage nodes organized as a data lake


![](./cloud-img/break.png)



# AWS Infrastructure - IaaS

## **VPC** - Amazon Virtual Private Cloud
Allows user to launch AWS resources from a virtual network. Customizable network configuration, public subnet for web servers and private subnet for backend systems(databases or app servers). Multiple layers of security control EC2 instances in each subnet.

Can be used to build a public and private subnet. Public subnet has EC2 instance that hosts a web application, the Private subnet has an RDS instance

Host simple websites or blog. 

Multi-tiwer web applications with strict access and security restrictions. Webservers in public subnet, application servers and databases in private subnets. 

Back up and recover data center into EC2 instances. Import virtual machine images.

Extend corporate network

## **EC2** - Amazon Elastic Cloud
Provides secure and resizable compute capacity in the cloud. Allows scaling up or down as needed.

When stopping an instance, the Amazon Elastic Block Store(EBS) will retain data. Charges for the volume used still incur, terminating the instance permanently deletes all data

On-demand instances charge only for the amount of time used. No  long term commitment or upfront payment. Ideal for short-term workloads that cannot be interrupted and low-cost computing

Spot Instances can be acquired at a 90% discount of On-demand. Ideal for workloads that can be paused and restarted.

Reserved Instances provide up tp 70% discount. This means you commit to paying for the instance for 1 to 3 years. Ideal for computing that needs steady usage amount.

## **RDS** - Amazon Relational Database Service

## **Route 53**

## **AS3** - Amazon Simple Storage Service

Object storage service
* scalable
* data availability
* security
* performance

Used to store and protect
* Websites
* Mobile apps
* backup and restore
* enterprise apps
* Inter of Things devices (IoT)
* big data analytics

AS3 data is stored in buckets
* Bucket name cannot be changed
* Private by default
* permissions → Object Ownership → enable ACLs 

**`ACLs`** - Access control list

**`ARN`** - Amazon Resource name

```aws
aws s3 ls - list buckets

aws s3 ls s3://bucketname - list contents of bucket
```

## Granting read/write access from EC2 instace to bucket


## **AWS Systems manager Session Manager**
* Review


# AWS Platform - PaaS

## **Elastic Beanstalk**
* Deploying and scaling web apps

![](./cloud-img/break.png)

# AWS Software - SaaS

Most services

![](./cloud-img/break.png)

# **Service Architecture**

## Monolithic architecture
Single point of failure causes all other components to fail

## Microservices architecture
Application components are loosely coupled. Single component failure does not affect the other components, preventing the entire application from failing


# **Types of Services**

## Managed services
Require managing tasks: patching, backup, repair. Grant virtual access to OS and servers. Customer is responsible for scaling and building for high availability

## Fully managed services
Automate ingrastructure management tasks. No grants to virtual access and underlying OS or servers. Customer still responsible for scaling and availability

## Serverless services
Services, practices, and strategies that you can use to build more agile applications. Faster response to change and innovation. Allows user to focus on application
<div style="display:flex; justify-content:space-between;">
<div>

Provides:
* Automatic scaling
* built-in high availability
* pay-for-value billing model
</div>
<div>

AWS handles infrastructure tasks 
* capacity provisioning
* patching 

</div>

</div>




![](./cloud-img/break.png)


# *Class 11 - 3/10/2022*

Spark - successor of Hadoop

Big data, cloud processing


Apache Hadoop lacks Unified Vision - most specialized systems **are not** loosely coupled

Apis dont have a unified format, and each system has their own Api

e.g. it is not easy to integrate MapReduce into Hive.

* Sparsely implemented modules
* Highlky diversified APIs. To run different systems, you will have to learn different systems.

## Spark Ecosystem : A Unified Pipeline

Easy to integrate code from one system into another. No need to maintain different system

### **Motivation**
### **MapReduce** - produced as a processing data engine.
* HDFS data stored in disks
* Input into MapReduce from cloud HDFS
* Output gets stored in cloud HDFS

### **Spark** General purpose computational framework that substantially improves performance of MapReduce, but retains the basic model
* Memory based data processing → avopids costly I/O by keeping intermediate results in memory
* Leverages distributed memory
* Leverages distributed memory
* Remembers operations applied to dataset

### Flexible, in-memory data processing framework written in Scala
**Goals:**
* Simplicity:
  * Rich APIs for Scala, Java and Python
* Generality: APIs for different types of workloads
  * Batch Streaming, Machine Learning, Graph
* Low Latency(Perfomance): In-memory processing and caching
* Fault-tolerance: Faults shouldn't be special case

## Resilient Distributed Dataset(RDD)
Fundamental uniot of dat ain Spark: An *Immutable* collection of objects(or record, or elements) that can be operated on "in parallel" (spread across a cluster)

**Resilient** - if data in memory is lost, it can be recreated
* Recover from node failures
* An RDD keeps its lineage information → it can be recreated from parent RDDs
  
**Distributed** - processed across the cluster
* Each RDD is composed of one or more partitiions → (partitions = parallelism)

**Dataset** - can be imported from file or be created

### RDDs
**Key Idea** - Write apps in terms of transofrmations on distributed datasets. One RDD per transformation
* Organize the RDDs into a DAG showing how data flows
* RDD can be saved *into memory* and resued or recomputed. Spark can save it to disk if the dataset does not fit in memory
* Built through parallel transformation (map, filter, group-by, join, etc). Automatically rebuilt on failure
* Controllable persistence(caching in RAM)

### RDDs are designed to be "Immutable"
* Create once, reuse without changes. Spark knows lineage → can be recreated at any time → Fault-tolerance
* Avoids data inconsistency problems( no simultaneous updates) → Correctness
* Easily live in memory as on disk → Caching → Safe to share across processes/tasks→ Improves performance
* Tradeoff: (**Fault-tolerance & Correctness**) vs (*DiskMemory & CPU*)

### Creating a RDD
Three ways to create a RDD
* From a file or set of files
* From another RDD(*most common*)
* From a database
* From cache/data in memory

### A File-based RDD
![](cloud-img/11-05.png)

### RDD Operations
Two types of operations **Transformations**: Define a new RDD, based on current RDDs

**Actions** : return values

![](cloud-img/11-06.png)

### RDD Transformations - Lazy evaluations
* Set of operations on a RDD that define how they should be transformed
* As in relational algebra, the application of a transformation to an RDD yields a new RDD(because RDD are immutable)
* Transformations are lazily evaluated, which allow for optimizations to take place before execution
* Examples: map(), filter(), groupBykey(), sortByKey()...